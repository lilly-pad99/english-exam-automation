#!/usr/bin/env python3
"""
í†µí•© ì˜ì–´ í•™ìŠµ ìë™í™” ì‹œìŠ¤í…œ
- Phase 2: ë‰´ìš•íƒ€ì„ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ
- Phase 3: AI ë²ˆì—­ ë° í•™ìŠµ ìë£Œ ìƒì„±
- Phase 4: Slack ì „ì†¡
- Slack ë´‡: ë‹¨ì–´ ì¡°íšŒ ë° ì‹œí—˜ì§€ ìƒì„±
"""

import os
import re
import json
import logging
import random
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import requests

# Web Scraping
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time

# Slack SDK
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler

# Google Sheets API
import gspread
from google.oauth2.service_account import Credentials

# AI APIs
import anthropic

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class NYTimesArticleExtractor:
    """ë‰´ìš•íƒ€ì„ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ í´ë˜ìŠ¤ (Phase 2)"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.driver = None

    def _get_full_article_text(self, url: str) -> str:
        """Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘"""
        try:
            if "/interactive/" in url or "/learning/" in url:
                logger.info(f"Skipping non-article URL: {url}")
                return ""

            logger.info(f"Seleniumìœ¼ë¡œ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")

            # --- Chrome ì˜µì…˜ ì„¤ì • ---
            chrome_options = Options()
            # Using a temporary profile to avoid permission issues.
            chrome_options.add_argument("--headless") # Run in headless mode
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--log-level=3") # Suppress console logs

            # --- WebDriver ì´ˆê¸°í™” ---
            # webdriver-manager will download and manage the driver automatically
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)

            driver.get(url)
            # Wait for the page to load dynamically
            time.sleep(10)

            # --- BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹± ---
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            driver.quit()

            # --- ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ---
            # NYT articles typically have their main content in a <section>
            # with the name "articleBody".
            article_body = soup.find('section', attrs={'name': 'articleBody'})
            if not article_body:
                # Fallback to other common selectors
                article_body = soup.find('div', class_='story-body') or soup.find('div', id='story')

            if not article_body:
                logger.warning("ê¸°ì‚¬ ë³¸ë¬¸(<section name='articleBody'>)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            # Find all paragraph tags within the article body
            paragraphs = article_body.find_all('p')
            full_text = "\n\n".join(p.get_text() for p in paragraphs)

            logger.info(f"ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ, ê¸€ì ìˆ˜: {len(full_text)}")
            return full_text

        except Exception as e:
            logger.error(f"Selenium ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            if 'driver' in locals() and driver:
                driver.quit()
            return ""

    def get_daily_topic(self) -> str:
        """ì¼ë³„ ì£¼ì œ ìˆœí™˜ (medical â†’ politics â†’ technology)"""
        topics = ['medical', 'politics', 'technology']
        day_of_year = datetime.now().timetuple().tm_yday
        topic = topics[day_of_year % 3]

        # ê°•ì œ ì£¼ì œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜)
        force_topic = os.getenv('FORCE_TOPIC', '').lower()
        if force_topic in topics:
            topic = force_topic

        return topic

    def extract_articles(self, topic: str, max_articles: int = 10) -> List[Dict]:
        """APIë¡œ ê¸°ì‚¬ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³ , ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            logger.info(f"ì£¼ì œ '{topic}'ì—ì„œ 4ê°œ ì´ìƒ ë¬¸ë‹¨ì´ ìˆëŠ” ê¸°ì‚¬ ì¶”ì¶œ ì‹œì‘")

            if not self.api_key:
                logger.error("NYT API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []

            # NYT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ê²€ìƒ‰
            url = f"https://api.nytimes.com/svc/search/v2/articlesearch.json?q={topic}&api-key={self.api_key}"
            response = requests.get(url)
            response.raise_for_status()
            data = response.json()

            if not data.get("response", {}).get("docs"):
                logger.warning("APIì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            for entry in data["response"]["docs"]:
                try:
                    article_url = entry.get('web_url')
                    if not article_url:
                        continue

                    # ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì „ì²´ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    full_content = self._get_full_article_text(article_url)
                    if not full_content:
                        logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {entry['headline']['main']}")
                        continue

                    # ë³¸ë¬¸ì„ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„ë¦¬
                    # Split by double newlines, which is how we joined them
                    paragraphs = [p.strip() for p in full_content.split('\n\n') if p.strip()]
                    word_count = len(full_content.split())


                    # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸ (ë¬¸ë‹¨ 4ê°œ ì´ìƒ, ë‹¨ì–´ 500ê°œ ì´ìƒ)
                    if len(paragraphs) >= 4 and word_count >= 500:
                        article = {
                            "selected_date": datetime.now().strftime("%Y-%m-%d"),
                            "daily_topic": topic,
                            "article": {
                                "title": entry['headline']['main'],
                                "link": article_url,
                                "topic": topic,
                                "published": entry['pub_date'],
                                "quality_score": 100, # Scraped articles are high quality
                                "content": {
                                    "paragraphs": paragraphs,
                                    "total_paragraphs": len(paragraphs),
                                    "word_count": word_count
                                }
                            }
                        }
                        logger.info(f"ì „ì²´ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {article['article']['title']}")
                        return [article] # ì²« ë²ˆì§¸ ê¸°ì¤€ ì¶©ì¡± ê¸°ì‚¬ ë°˜í™˜

                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            logger.warning("ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ë¬¸ë‹¨ 4ê°œ ì´ìƒ, ë‹¨ì–´ 500ê°œ ì´ìƒ).")
            return []

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    

class AIArticleProcessor:
    """AI ê¸°ì‚¬ ì²˜ë¦¬ í´ë˜ìŠ¤ (Phase 3)"""
    
    def __init__(self, anthropic_api_key: str):
        self.client = anthropic.Anthropic(api_key=anthropic_api_key)
    
    def process_article(self, article_data: Dict) -> Dict:
        """ê¸°ì‚¬ë¥¼ í•™ìŠµ ìë£Œë¡œ ì²˜ë¦¬"""
        try:
            logger.info("Phase 3: AI ì²˜ë¦¬ ì‹œì‘")
            
            article = article_data.get('article', {})
            paragraphs = article.get('content', {}).get('paragraphs', [])
            
            if not paragraphs:
                return {'success': False, 'error': 'ë¬¸ë‹¨ì´ ë¶€ì¡±í•©ë‹ˆë‹¤'}
            
            # ë²ˆì—­í•  ë¬¸ë‹¨ ì„ íƒ (ë§ˆì§€ë§‰ 2ê°œ)
            translation_indices = self._select_translation_paragraphs(paragraphs, 2)
            
            # í˜¼í•© ì½˜í…ì¸  ìƒì„±
            mixed_content = self._create_mixed_content(paragraphs, translation_indices, article)
            
            # í•´ì„¤ ìë£Œ ìƒì„±
            commentary_data = self._create_commentary_data(paragraphs, translation_indices, article)
            
            # íŒŒì¼ ì €ì¥
            mixed_file = self._save_mixed_content(mixed_content)
            commentary_file = self._save_commentary_data(commentary_data)
            
            return {
                'success': True,
                'article_title': article.get('title', ''),
                'topic': article.get('topic', ''),
                'mixed_content_file': mixed_file,
                'commentary_file': commentary_file,
                'mixed_content_data': mixed_content,
                'commentary_data': commentary_data
            }
            
        except Exception as e:
            logger.error(f"AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _select_translation_paragraphs(self, paragraphs: List[str], count: int = 2) -> List[int]:
        """ë²ˆì—­í•  ë¬¸ë‹¨ ì„ íƒ (ë§ˆì§€ë§‰ 2ê°œ)"""
        total = len(paragraphs)
        if total <= count:
            return list(range(total))
        
        return list(range(total - count, total))
    
    def _create_mixed_content(self, paragraphs: List[str], translation_indices: List[int], article: Dict) -> Dict:
        """í˜¼í•© ì½˜í…ì¸  ìƒì„± (ì˜ì–´ + í•œê¸€)"""
        mixed_paragraphs = []
        
        for i, paragraph in enumerate(paragraphs):
            if i in translation_indices:
                # í•œê¸€ ë²ˆì—­
                korean_text = self._translate_to_korean(paragraph, article.get('topic', ''))
                mixed_paragraphs.append({
                    'paragraph_number': i + 1,
                    'type': 'korean',
                    'content': korean_text
                })
            else:
                # ì˜ì–´ ì›ë¬¸
                mixed_paragraphs.append({
                    'paragraph_number': i + 1,
                    'type': 'english',
                    'content': paragraph
                })
        
        korean_indices = [i + 1 for i in translation_indices]
        
        return {
            "title": article.get('title', ''),
            "source": "New York Times",
            "topic": article.get('topic', ''),
            "published_date": article.get('published', ''),
            "processing_date": datetime.now().isoformat(),
            "content_structure": {
                "total_paragraphs": len(mixed_paragraphs),
                "english_paragraphs": len(paragraphs) - len(translation_indices),
                "korean_paragraphs": len(translation_indices),
                "translation_indices": korean_indices
            },
            "paragraphs": mixed_paragraphs,
            "reading_instruction": "ì˜ì–´ ë¬¸ë‹¨ì€ ì´í•´í•˜ë©° ì½ê³ , í•œê¸€ ë¬¸ë‹¨ì€ ì˜ì–´ë¡œ ë²ˆì—­í•´ë³´ì„¸ìš”."
        }
    
    def _create_commentary_data(self, paragraphs: List[str], translation_indices: List[int], article: Dict) -> Dict:
        """í•´ì„¤ ë° í†µì—­ ì—°ìŠµ ìë£Œ ìƒì„±"""
        # ì˜ì–´ ë¬¸ë‹¨ì—ì„œ í‘œí˜„ ì¶”ì¶œ
        english_paragraphs = [p for i, p in enumerate(paragraphs) if i not in translation_indices]
        expressions = self._extract_expressions(' '.join(english_paragraphs), article.get('topic', ''))
        
        # í•œê¸€ ë¬¸ë‹¨ìœ¼ë¡œ í†µì—­ ì—°ìŠµ ìƒì„±
        translation_exercises = []
        for i in translation_indices:
            if i < len(paragraphs):
                korean_text = self._translate_to_korean(paragraphs[i], article.get('topic', ''))
                exercise = self._create_translation_exercise(korean_text, i + 1, article)
                translation_exercises.append(exercise)
        
        return {
            "title": f"í•´ì„¤ ë° í†µì—­ ì—°ìŠµ - {article.get('title', '')}",
            "source_article": {
                "title": article.get('title', ''),
                "topic": article.get('topic', ''),
                "url": article.get('link', '')
            },
            "processing_date": datetime.now().isoformat(),
            "part_1_expressions": {
                "description": "ì›ë¬¸ì—ì„œ ì¶”ì¶œí•œ ì¤‘ìš” ì˜ì–´ í‘œí˜„ 10ê°œ",
                "expressions": expressions[:10]
            },
            "part_2_translation_practice": {
                "description": "í•œê¸€ ë¬¸ë‹¨ì˜ í†µì—­ ì—°ìŠµ (í•œâ†’ì˜)",
                "korean_paragraphs": [i + 1 for i in translation_indices],
                "translation_exercises": translation_exercises
            }
        }
    
    def _translate_to_korean(self, text: str, context: str) -> str:
        """ì˜ì–´ë¥¼ í•œê¸€ë¡œ ë²ˆì—­"""
        try:
            prompt = f"""
ë‹¤ìŒ ì˜ì–´ ë¬¸ë‹¨ì„ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {context}
ì›ë¬¸: {text}

ë²ˆì—­í•  ë•Œ ê³ ë ¤ì‚¬í•­:
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ í‘œí˜„ ì‚¬ìš©
- ì „ë¬¸ ìš©ì–´ëŠ” ì ì ˆí•œ í•œêµ­ì–´ ìš©ì–´ë¡œ ë²ˆì—­
- ë¬¸ë§¥ê³¼ ë‰˜ì•™ìŠ¤ ìœ ì§€
- ì½ê¸° ì‰¬ìš´ ë¬¸ì¥ êµ¬ì¡°ë¡œ ë²ˆì—­

ë²ˆì—­ë¬¸ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=500,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return response.content[0].text.strip()
            
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return text  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ë°˜í™˜
    
    def _extract_expressions(self, text: str, topic: str) -> List[Dict]:
        """ì˜ì–´ í‘œí˜„ ì¶”ì¶œ"""
        try:
            prompt = f"""
ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•˜ê³  ìœ ìš©í•œ í‘œí˜„ 10ê°œë¥¼ ì¶”ì¶œí•˜ì—¬ JSON ë°°ì—´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.

ì£¼ì œ: {topic}
í…ìŠ¤íŠ¸: {text}

ê° í‘œí˜„ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
{{
    "expression": "ì¶”ì¶œëœ í‘œí˜„",
    "korean_meaning": "í•œê¸€ ì˜ë¯¸",
    "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ë™ì˜ì–´3"],
    "context": "ì›ë¬¸ì—ì„œì˜ ì‚¬ìš© ì˜ˆ",
    "usage_note": "ì‚¬ìš©ë²• ì„¤ëª…",
    "formality": "ê²©ì‹ë„ (ê²©ì‹ì²´/ë¹„ê²©ì‹ì²´/ì „ë¬¸ìš©ì–´)"
}}

ì‘ë‹µì€ JSON ë°°ì—´ í˜•íƒœë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=2000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = response.content[0].text.strip()
            
            # JSON íŒŒì‹±
            try:
                json_str = content
                if "```json" in json_str:
                    json_str = json_str.split("```json")[1].split("```")[0]
                elif "```" in json_str:
                    json_str = json_str.split("```")[1].split("```")[0]
                
                start_index = json_str.find('[')
                end_index = json_str.rfind(']')
                
                if start_index != -1 and end_index != -1:
                    json_str = json_str[start_index:end_index+1]
                    return json.loads(json_str)
                
                logger.warning("Could not find JSON array in the response.")
                return []

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON: {e}")
                logger.error(f"Response content: {content}")
                return []
            
        except Exception as e:
            logger.error(f"í‘œí˜„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _create_translation_exercise(self, korean_text: str, paragraph_number: int, article: Dict) -> Dict:
        """í†µì—­ ì—°ìŠµ ìƒì„±"""
        try:
            escaped_korean_text = json.dumps(korean_text, ensure_ascii=False)
            
            prompt = f"""
ë‹¤ìŒ í•œê¸€ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í†µì—­ ì—°ìŠµì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ì£¼ì œ: {article.get('topic', '')}
í•œê¸€ ë¬¸ì¥: {korean_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSON ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "paragraph_number": {paragraph_number},
    "korean_text": {escaped_korean_text},
    "interpretation_approach": "í†µì—­ ê´€ì ì—ì„œì˜ ë²ˆì—­ ì ‘ê·¼ë²•",
    "key_challenges": ["ë²ˆì—­ ì‹œ ì£¼ì˜í•  ì 1", "ì£¼ì˜í•  ì 2", "ì£¼ì˜í•  ì 3"],
    "professional_translation": "ëª¨ë²” ë²ˆì—­ë¬¸",
    "alternative_versions": ["ëŒ€ì•ˆ ë²ˆì—­1", "ëŒ€ì•ˆ ë²ˆì—­2"],
    "interpretation_notes": ["í†µì—­ íŒ1", "í†µì—­ íŒ2", "í†µì—­ íŒ3"]
}}
"""

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=1000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = response.content[0].text.strip()
            
            # JSON íŒŒì‹±
            try:
                json_str = content
                if "```json" in json_str:
                    json_str = json_str.split("```json")[1].split("```")[0]
                elif "```" in json_str:
                    json_str = json_str.split("```")[1].split("```")[0]
                
                start_index = json_str.find('{')
                end_index = json_str.rfind('}')
                
                if start_index != -1 and end_index != -1:
                    json_str = json_str[start_index:end_index+1]
                    return json.loads(json_str)
                
                logger.warning("Could not find JSON object in the response.")
                return {
                    "paragraph_number": paragraph_number,
                    "korean_text": korean_text,
                    "professional_translation": "Translation exercise could not be generated"
                }

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON: {e}")
                logger.error(f"Response content: {content}")
                return {
                    "paragraph_number": paragraph_number,
                    "korean_text": korean_text,
                    "professional_translation": "Translation exercise could not be generated"
                }
            
        except Exception as e:
            logger.error(f"í†µì—­ ì—°ìŠµ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "paragraph_number": paragraph_number,
                "korean_text": korean_text,
                "professional_translation": "Translation exercise could not be generated"
            }
    
    def _save_mixed_content(self, content: Dict) -> str:
        """í˜¼í•© ì½˜í…ì¸  íŒŒì¼ ì €ì¥"""
        os.makedirs('output', exist_ok=True)
        filename = f"output/mixed_content_{content.get('topic', 'general')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def _save_commentary_data(self, content: Dict) -> str:
        """í•´ì„¤ ìë£Œ íŒŒì¼ ì €ì¥"""
        os.makedirs('output', exist_ok=True)
        filename = f"output/commentary_{content.get('source_article', {}).get('topic', 'general')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=2)
        
        return filename

class SlackNotifier:
    """Slack ì „ì†¡ í´ë˜ìŠ¤ (Phase 4)"""
    
    def __init__(self, bot_token: str, channel: str):
        self.client = WebClient(token=bot_token)
        self.channel = channel
    
    def send_daily_materials(self, mixed_content_data: Dict, commentary_data: Dict) -> Dict:
        """ì¼ì¼ í•™ìŠµ ìë£Œ ì „ì†¡"""
        try:
            results = {
                'mixed_content_sent': False,
                'commentary_sent': False,
                'errors': []
            }
            
            # 1. í˜¼í•© ì½˜í…ì¸  ì „ì†¡
            mixed_messages = self._format_mixed_content_message(mixed_content_data)
            try:
                for message in mixed_messages:
                    response = self.client.chat_postMessage(
                        channel=self.channel,
                        **message
                    )
                results['mixed_content_sent'] = True
                logger.info("í˜¼í•© ì½˜í…ì¸  ì „ì†¡ ì™„ë£Œ")
            except SlackApiError as e:
                logger.error(f"í˜¼í•© ì½˜í…ì¸  ì „ì†¡ ì‹¤íŒ¨: {e.response['error']}")
                results['errors'].append(f"í˜¼í•© ì½˜í…ì¸  ì „ì†¡ ì‹¤íŒ¨: {e}")
            
            # 2. í•´ì„¤ ìë£Œ ì „ì†¡
            commentary_message = self._format_commentary_message(commentary_data)
            try:
                response = self.client.chat_postMessage(
                    channel=self.channel,
                    **commentary_message
                )
                results['commentary_sent'] = True
                logger.info("í•´ì„¤ ìë£Œ ì „ì†¡ ì™„ë£Œ")
            except SlackApiError as e:
                logger.error(f"í•´ì„¤ ìë£Œ ì „ì†¡ ì‹¤íŒ¨: {e.response['error']}")
                results['errors'].append(f"í•´ì„¤ ìë£Œ ì „ì†¡ ì‹¤íŒ¨: {e}")
            
            logger.info(f"Slack ì „ì†¡ ê²°ê³¼: {results}")
            return results
            
        except Exception as e:
            logger.error(f"Slack ì „ì†¡ ì‹¤íŒ¨: {e}")
            return {'mixed_content_sent': False, 'commentary_sent': False, 'errors': [str(e)]}
    
    def _format_mixed_content_message(self, data: Dict) -> List[Dict]:
        """í˜¼í•© ì½˜í…ì¸  ë©”ì‹œì§€ í¬ë§·íŒ…"""
        title = data.get('title', '')
        topic = data.get('topic', '').title()
        published_date = data.get('published_date', '')[:10]
        
        header_text = (
            f"ğŸ“° *ì˜¤ëŠ˜ì˜ ì˜ì–´ ê¸°ì‚¬* ({topic})\n"
            f"*ì œëª©:* {title}\n"
            f"*ì¶œì²˜:* New York Times | *ë‚ ì§œ:* {published_date}\n\n"
        )
        
        content_text = "=" * 50 + "\n\n"
        
        for para in data.get('paragraphs', []):
            para_num = para.get('paragraph_number', 0)
            para_type = para.get('type', 'english')
            
            if para_type == 'korean':
                content_text += f"*ğŸ‡°ğŸ‡· ë¬¸ë‹¨ {para_num} (í•œê¸€)*\n"
                content_text += f"{para.get('content', '')}\n\n"
            else:
                content_text += f"*ğŸ“– ë¬¸ë‹¨ {para_num} (ì˜ì–´)*\n"
                content_text += f"{para.get('content', '')}\n\n"
        
        structure = data.get('content_structure', {})
        footer_text = (
            "=" * 50 + "\n"
            f"ğŸ’¡ *í•™ìŠµ íŒ:* ì˜ì–´ ë¬¸ë‹¨ì€ ì´í•´í•˜ë©° ì½ê³ , í•œê¸€ ë¬¸ë‹¨ì€ ì˜ì–´ë¡œ ë²ˆì—­í•´ë³´ì„¸ìš”!\n"
            f"ğŸ“Š *êµ¬ì„±:* ì´ {structure.get('total_paragraphs', 0)}ë¬¸ë‹¨ ì¤‘ "
            f"ì˜ì–´ {structure.get('english_paragraphs', 0)}ê°œ, "
            f"í•œê¸€ {structure.get('korean_paragraphs', 0)}ê°œ"
        )
        
        full_text = header_text + content_text + footer_text
        
        # Split the message into chunks of 2500 characters
        chunks = []
        while len(full_text) > 2500:
            split_pos = full_text.rfind("\n\n", 0, 2500)
            if split_pos == -1:
                split_pos = 2500
            chunks.append(full_text[:split_pos])
            full_text = full_text[split_pos:]
        chunks.append(full_text)
        
        messages = []
        for chunk in chunks:
            messages.append({
                "text": chunk,
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": chunk
                        }
                    }
                ]
            })
        
        return messages
    
    def _format_commentary_message(self, data: Dict) -> Dict:
        """í•´ì„¤ ìë£Œ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        expressions = data.get('part_1_expressions', {}).get('expressions', [])[:10]
        
        header_text = (
            f"ğŸ“š *ì˜¤ëŠ˜ì˜ ì˜ì–´ í‘œí˜„ í•´ì„¤*\n"
            f"*ì¶œì²˜:* {data.get('source_article', {}).get('title', '')}\n\n"
            f"*ğŸ¯ ì›ë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‘œí˜„*\n\n"
        )
        
        expressions_text = ""
        
        emojis = ["1ï¸âƒ£", "2ï¸âƒ£", "3ï¸âƒ£", "4ï¸âƒ£", "5ï¸âƒ£", "6ï¸âƒ£", "7ï¸âƒ£", "8ï¸âƒ£", "9ï¸âƒ£", "ğŸ”Ÿ"]
        for i, expr in enumerate(expressions, 1):
            if i > len(emojis):
                break
            emoji_num = emojis[i-1]
            expressions_text += f"*{emoji_num} {expr.get('expression', '')}*\n"
            expressions_text += f"ğŸ‡°ğŸ‡· *ëœ»:* {expr.get('korean_meaning', '')}\n"
            
            if expr.get('synonyms'):
                synonyms_text = ', '.join(expr['synonyms'][:2])
                expressions_text += f"ğŸ”„ *ë™ì˜ì–´:* {synonyms_text}\n"
            
            if expr.get('context'):
                context_preview = expr['context'][:60] + "..." if len(expr['context']) > 60 else expr['context']
                expressions_text += f"ğŸ“ *ì˜ˆë¬¸:* {context_preview}\n"
            
            expressions_text += "\n"
        
        full_text = header_text + expressions_text
        
        return {
            "text": full_text,
            "blocks": [
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": full_text
                    }
                }
            ]
        }

class VocabBot:
    """ì˜ì–´ ë‹¨ì–´ ë´‡ í´ë˜ìŠ¤"""
    
    def __init__(self, slack_bot_token: str, slack_app_token: str, 
                 anthropic_api_key: str, google_service_account_file: str,
                 spreadsheet_id: str):
        
        # Slack ì„¤ì •
        self.app = App(token=slack_bot_token)
        self.client = WebClient(token=slack_bot_token)
        self.socket_handler = SocketModeHandler(self.app, slack_app_token)
        
        # Claude ì„¤ì •
        self.claude_client = anthropic.Anthropic(api_key=anthropic_api_key)
        
        # Google Sheets ì„¤ì •
        self.setup_google_sheets(google_service_account_file, spreadsheet_id)
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡
        self.register_handlers()
    
    def setup_google_sheets(self, service_account_file: str, spreadsheet_id: str):
        """Google Sheets ì—°ê²° ì„¤ì •"""
        try:
            scope = [
                'https://spreadsheets.google.com/feeds',
                'https://www.googleapis.com/auth/drive'
            ]
            
            creds = Credentials.from_service_account_file(
                service_account_file, scopes=scope
            )
            self.gc = gspread.authorize(creds)
            self.spreadsheet = self.gc.open_by_key(spreadsheet_id)
            self.worksheet = self.spreadsheet.worksheet('ì‹œíŠ¸1')
            
            logger.info("Google Sheets ì—°ê²° ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"Google Sheets ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def register_handlers(self):
        """ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡"""
        
        @self.app.message(re.compile(r"^@(\S+)"))
        def handle_vocab_query(message, say, context):
            """@ë‹¨ì–´ ëª…ë ¹ì–´ ì²˜ë¦¬"""
            try:
                word = context['matches'][0].strip()
                
                if not word:
                    say("âŒ ë‹¨ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: @hello")
                    return
                
                say(f"ğŸ” '{word}' ê²€ìƒ‰ ì¤‘...")
                
                # ë‹¨ì–´ ì •ë³´ ì¡°íšŒ
                word_info = self.get_word_definition(word)
                
                if not word_info:
                    say(f"âŒ '{word}'ì˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return
                
                # Google Sheetsì— ì¶”ê°€
                success = self.add_to_sheet(word_info)
                
                # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
                result_message = self.format_vocab_response(word_info, success)
                say(result_message)
                
            except Exception as e:
                logger.error(f"ë‹¨ì–´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                say(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        @self.app.message(re.compile(r"(?:^ì‹œí—˜ì§€|^test paper)\s*(\d+)?(?:\s+s1=(\d+))?(?:\s+s2=(\d+))?(?:\s+s3=(\d+))?(?:\s+s4=(\d+))?(?:\s+s5=(\d+))?"))
        def handle_exam_generation(message, say, context):
            """ì‹œí—˜ì§€ ìƒì„± ëª…ë ¹ì–´ ì²˜ë¦¬"""
            try:
                matches = context['matches']
                total_count_str = matches[0]
                s1_str = matches[1]
                s2_str = matches[2]
                s3_str = matches[3]
                s4_str = matches[4]
                s5_str = matches[5]

                total_count = int(total_count_str) if total_count_str else 30
                s1 = int(s1_str) if s1_str else 15
                s2 = int(s2_str) if s2_str else 15
                s3 = int(s3_str) if s3_str else 5
                s4 = int(s4_str) if s4_str else 5
                s5 = int(s5_str) if s5_str else 10
                
                if total_count < 10 or total_count > 100:
                    say("âŒ ë¬¸ì œ ìˆ˜ëŠ” 10~100ê°œ ì‚¬ì´ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: ì‹œí—˜ì§€ 30")
                    return
                
                say(f"ğŸ“ {total_count}ë¬¸ì œ ì‹œí—˜ì§€ ìƒì„± ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                
                # ì‹œí—˜ì§€ ìƒì„±
                exam_result = self.generate_exam(total_count, s1, s2, s3, s4, s5)
                
                if exam_result['success']:
                    # ì‹œí—˜ì§€ íŒŒì¼ ì—…ë¡œë“œ
                    self.upload_exam_files(exam_result, message['channel'])
                    
                    result_message = f"""
ğŸ“„ **ì‹œí—˜ì§€ ìƒì„± ì™„ë£Œ!**

ğŸ“Š **êµ¬ì„±:**
â€¢ Section 1: ì˜â†’í•œ ë²ˆì—­ ({exam_result['section1_count']}ë¬¸ì œ)
â€¢ Section 2: í•œâ†’ì˜ ë²ˆì—­ ({exam_result['section2_count']}ë¬¸ì œ)
â€¢ Section 3: ì˜ì–´ ì‘ë¬¸ ({exam_result['section3_count']}ë¬¸ì œ)
â€¢ Section 4: ë¬¸ë§¥ ë²ˆì—­ ({exam_result['section4_count']}ë¬¸ì œ)
â€¢ Section 5: ë™ì˜ì–´ ì„ íƒ ({exam_result['section5_count']}ë¬¸ì œ)

**ì´ {exam_result['total_questions']}ë¬¸ì œ**

âœ… ì‹œí—˜ì§€ì™€ ë‹µì§€ê°€ ë³„ë„ íŒŒì¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
                    """
                    say(result_message)
                else:
                    say(f"âŒ ì‹œí—˜ì§€ ìƒì„± ì‹¤íŒ¨: {exam_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
            except ValueError:
                say("âŒ ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: ì‹œí—˜ì§€ 30")
            except Exception as e:
                logger.error(f"ì‹œí—˜ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                say(f"âŒ ì‹œí—˜ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        @self.app.message("ë„ì›€ë§")
        def handle_help(say):
            """ë„ì›€ë§ ë©”ì‹œì§€"""
            help_text = """
ğŸ“š *ì˜ì–´ ë‹¨ì–´ ë´‡ ì‚¬ìš©ë²•*

*ëª…ë ¹ì–´:*
â€¢ `@ë‹¨ì–´` - ì˜ì–´ ë‹¨ì–´ ëœ» ì¡°íšŒ ë° ì €ì¥
â€¢ `ì‹œí—˜ì§€` ë˜ëŠ” `ì‹œí—˜ì§€ 30` - ì‹œí—˜ì§€ ìƒì„± (ê¸°ë³¸ 30ë¬¸ì œ)
â€¢ `ë„ì›€ë§` - ì´ ë©”ì‹œì§€ í‘œì‹œ

*ì˜ˆì‹œ:*
â€¢ `@hello` - hello ë‹¨ì–´ ì¡°íšŒ
â€¢ `@beautiful` - beautiful ë‹¨ì–´ ì¡°íšŒ
â€¢ `@take care of` - êµ¬ë¬¸ ì¡°íšŒ
â€¢ `ì‹œí—˜ì§€` - 30ë¬¸ì œ ì‹œí—˜ì§€ ìƒì„±
â€¢ `ì‹œí—˜ì§€ 50` - 50ë¬¸ì œ ì‹œí—˜ì§€ ìƒì„±

*ë‹¨ì–´ ì¡°íšŒ ê¸°ëŠ¥:*
âœ… ì˜ì–´ ë‹¨ì–´/êµ¬ë¬¸ ì˜ë¯¸ ì¡°íšŒ
âœ… ë™ì˜ì–´ ì œê³µ
âœ… ì˜ˆë¬¸ ì œê³µ
âœ… Google Sheets ìë™ ì €ì¥
âœ… ì¤‘ë³µ ê²€ì‚¬

*ì‹œí—˜ì§€ ìƒì„± ê¸°ëŠ¥:*
âœ… Section 1: ì˜â†’í•œ ë²ˆì—­
âœ… Section 2: í•œâ†’ì˜ ë²ˆì—­  
âœ… Section 3: ì˜ì–´ ì‘ë¬¸
âœ… Section 4: ë¬¸ë§¥ ë²ˆì—­
âœ… Section 5: ë™ì˜ì–´ ì„ íƒ
âœ… ë‹µì§€ ìë™ ìƒì„±

*ì €ì¥ë˜ëŠ” ì •ë³´:*
â€¢ ì˜ì–´ í‘œí˜„
â€¢ í•œê¸€ ì˜ë¯¸
â€¢ ë™ì˜ì–´
            """
            say(help_text)
    
    def get_word_definition(self, word: str) -> Optional[Dict]:
        """Claude APIë¥¼ í†µí•œ ë‹¨ì–´ ì •ë³´ ì¡°íšŒ"""
        try:
            prompt = f"""
ë‹¤ìŒ ì˜ì–´ ë‹¨ì–´/í‘œí˜„ì— ëŒ€í•´ ì •í™•í•œ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”:

ë‹¨ì–´: "{word}"

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "word": "ë‹¨ì–´ ì›í˜•",
    "korean_meaning": "ì£¼ìš” í•œê¸€ ì˜ë¯¸",
    "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ë™ì˜ì–´3"],
    "example": "ì˜ì–´ ì˜ˆë¬¸",
    "korean_example": "ì˜ˆë¬¸ í•œê¸€ ë²ˆì—­"
}}

ì£¼ì˜ì‚¬í•­:
- ê°€ì¥ ì¼ë°˜ì ì´ê³  ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ì œê³µí•˜ì„¸ìš”
- ë™ì˜ì–´ëŠ” ì‹¤ìš©ì ì¸ ê²ƒë“¤ë¡œ ìµœëŒ€ 3ê°œê¹Œì§€
- ì˜ˆë¬¸ì€ ì‹¤ìƒí™œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ
- êµ¬ë¬¸ì˜ ê²½ìš° ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
- JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”
"""

            response = self.claude_client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=1000,
                temperature=0.3,
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = response.content[0].text.strip()
            
            # JSON íŒŒì‹±
            try:
                json_str = content
                if "```json" in json_str:
                    json_str = json_str.split("```json")[1].split("```")[0]
                elif "```" in json_str:
                    json_str = json_str.split("```")[1].split("```")[0]
                
                start_index = json_str.find('{')
                end_index = json_str.rfind('}')
                
                if start_index != -1 and end_index != -1:
                    json_str = json_str[start_index:end_index+1]
                    word_info = json.loads(json_str)
                    
                    required_fields = ['word', 'korean_meaning']
                    if not all(field in word_info for field in required_fields):
                        logger.error(f"Missing required fields: {word_info}")
                        return None
                    
                    if not word_info.get('synonyms'):
                        word_info['synonyms'] = []
                    
                    logger.info(f"Received word info from Claude: {word_info.get('word', '')}")
                    return word_info
                
                logger.warning("Could not find JSON object in the response.")
                return None

            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse JSON: {e}")
                logger.error(f"Response content: {content}")
                return None
            
        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_random_words(self, count: int) -> List[Dict]:
        """Google Sheetsì—ì„œ ëœë¤ ë‹¨ì–´ ì¶”ì¶œ"""
        try:
            all_data = self.worksheet.get_all_values()[1:]  # í—¤ë” ì œì™¸
            
            if len(all_data) < count:
                logger.warning(f"ìš”ì²­í•œ ë‹¨ì–´ ìˆ˜({count})ê°€ ì €ì¥ëœ ë‹¨ì–´ ìˆ˜({len(all_data)})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
                count = len(all_data)
            
            selected_rows = random.sample(all_data, count)
            
            words = []
            for row in selected_rows:
                if len(row) >= 3:
                    word_data = {
                        'word': row[0],
                        'meaning': row[1],
                        'synonyms': row[2].split(', ') if row[2] else []
                    }
                    words.append(word_data)
            
            logger.info(f"ëœë¤ ë‹¨ì–´ {len(words)}ê°œ ì„ íƒ ì™„ë£Œ")
            return words
            
        except Exception as e:
            logger.error(f"ëœë¤ ë‹¨ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_exam(self, total_count: int = 30, section1_count: int = 15, section2_count: int = 15, section3_count: int = 5, section4_count: int = 5, section5_count: int = 10) -> Dict:
        """ì‹œí—˜ì§€ ìƒì„±"""
        try:
            words = self.get_random_words(total_count)
            if len(words) < 10:
                return {
                    'success': False,
                    'error': f'ì €ì¥ëœ ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í˜„ì¬: {len(words)}ê°œ, ìµœì†Œ: 10ê°œ í•„ìš”)'
                }
            
            # ë‹¨ì–´ ë¶„ë°°
            section1_words = words[:section1_count]
            section2_words = words[section1_count:total_count] if total_count >= 30 else words[section1_count:]
            if len(section2_words) < section2_count:
                section2_words.extend(words[:section2_count - len(section2_words)])
            section2_words = section2_words[:section2_count]
            
            # ì‘ë¬¸ ë° ë¬¸ë§¥ìš© ë‹¨ì–´ (section1, 2ì—ì„œ ì¬ì‚¬ìš©)
            composition_words = (section1_words + section2_words)[:section3_count]
            context_words = (section1_words + section2_words)[section3_count:section3_count + section4_count]
            synonym_words = [w for w in (section1_words + section2_words) if w['synonyms']][:section5_count]
            
            # ì‹œí—˜ì§€ ìƒì„±
            exam_content = self.create_exam_content(
                section1_words, section2_words, composition_words, 
                context_words, synonym_words
            )
            
            answer_content = self.create_answer_content(
                section1_words, section2_words, composition_words,
                context_words, synonym_words
            )
            
            return {
                'success': True,
                'exam_content': exam_content,
                'answer_content': answer_content,
                'section1_count': section1_count,
                'section2_count': section2_count,
                'section3_count': section3_count,
                'section4_count': section4_count,
                'section5_count': len(synonym_words),
                'total_questions': section1_count + section2_count + section3_count + section4_count + len(synonym_words)
            }
            
        except Exception as e:
            logger.error(f"ì‹œí—˜ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def create_exam_content(self, section1_words, section2_words, composition_words, 
                           context_words, synonym_words) -> str:
        """ì‹œí—˜ì§€ ë‚´ìš© ìƒì„±"""
        content = f"""
# ì˜ì–´ ì‹¤ë ¥ í–¥ìƒ ì‹œí—˜ì§€
**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}
**ì´ ë¬¸í•­ìˆ˜**: {len(section1_words) + len(section2_words) + 5 + 5 + len(synonym_words)}ë¬¸ì œ

---

## Section 1: ì˜â†’í•œ ë²ˆì—­ ({len(section1_words)}ë¬¸ì œ)
*ë‹¤ìŒ ì˜ì–´ ë‹¨ì–´/í‘œí˜„ì˜ í•œê¸€ ëœ»ì„ ì“°ì‹œì˜¤.*

"""
        
        for i, word in enumerate(section1_words, 1):
            content += f"{i}. {word['word']}\n"
            content += f"   ë‹µ: ________________________\n\n"
        
        content += f"""
---

## Section 2: í•œâ†’ì˜ ë²ˆì—­ ({len(section2_words)}ë¬¸ì œ)
*ë‹¤ìŒ í•œê¸€ ëœ»ì— í•´ë‹¹í•˜ëŠ” ì˜ì–´ ë‹¨ì–´/í‘œí˜„ì„ ì“°ì‹œì˜¤.*

"""
        
        for i, word in enumerate(section2_words, 1):
            content += f"{i}. {word['meaning']}\n"
            content += f"   ë‹µ: ________________________\n\n"
        
        content += f"""
---

## Section 3: ì˜ì–´ ì‘ë¬¸ (5ë¬¸ì œ)
*ì œì‹œëœ ë‹¨ì–´ë¥¼ í™œìš©í•˜ì—¬ ì˜ì–´ ë¬¸ì¥ì„ ë§Œë“œì‹œì˜¤.*

"""
        
        for i, word in enumerate(composition_words[:5], 1):
            content += f"{i}. ì œì‹œì–´: **{word['word']}**\n"
            content += f"   ë¬¸ì¥: ________________________________________________\n\n"
        
        content += f"""
---

## Section 4: ë¬¸ë§¥ ë²ˆì—­ (5ë¬¸ì œ)
*ë‹¤ìŒ ìƒí™©ì—ì„œ ì œì‹œëœ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ ì˜ì–´ í‘œí˜„ì„ ì“°ì‹œì˜¤.*

"""
        
        contexts = [
            "ë¹„ì¦ˆë‹ˆìŠ¤ íšŒì˜ì—ì„œ",
            "ì¹œêµ¬ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ", 
            "ê³µì‹ì ì¸ ì´ë©”ì¼ì—ì„œ",
            "ì¹´í˜ì—ì„œ ì£¼ë¬¸í•  ë•Œ",
            "ì—¬í–‰ ì¤‘ í˜¸í…”ì—ì„œ"
        ]
        
        for i, (word, context) in enumerate(zip(context_words[:5], contexts), 1):
            content += f"{i}. ìƒí™©: {context}\n"
            content += f"   ë‹¨ì–´: **{word['word']}**\n"
            content += f"   í‘œí˜„: ________________________________________________\n\n"
        
        if synonym_words:
            content += f"""
---

## Section 5: ë™ì˜ì–´ ì„ íƒ ({len(synonym_words)}ë¬¸ì œ)
*ë‹¤ìŒ ë‹¨ì–´ì˜ ë™ì˜ì–´ë¥¼ ëª¨ë‘ ì“°ì‹œì˜¤.*

"""
            
            for i, word in enumerate(synonym_words, 1):
                content += f"{i}. **{word['word']}**ì˜ ë™ì˜ì–´ (ìµœì†Œ 2ê°œ):\n"
                content += f"   ë‹µ: ________________________________________________\n\n"
        
        return content
    
    def create_answer_content(self, section1_words, section2_words, composition_words,
                             context_words, synonym_words) -> str:
        """ë‹µì§€ ë‚´ìš© ìƒì„±"""
        content = f"""
# ì˜ì–´ ì‹¤ë ¥ í–¥ìƒ ì‹œí—˜ì§€ - ì •ë‹µ
**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}

---

## Section 1: ì˜â†’í•œ ë²ˆì—­ ì •ë‹µ

"""
        
        for i, word in enumerate(section1_words, 1):
            content += f"{i}. {word['word']} â†’ **{word['meaning']}**\n"
        
        content += f"""
---

## Section 2: í•œâ†’ì˜ ë²ˆì—­ ì •ë‹µ

"""
        
        for i, word in enumerate(section2_words, 1):
            content += f"{i}. {word['meaning']} â†’ **{word['word']}**\n"
        
        content += f"""
---

## Section 3: ì˜ì–´ ì‘ë¬¸ ì˜ˆì‹œ ë‹µì•ˆ

"""
        
        for i, word in enumerate(composition_words[:5], 1):
            content += f"{i}. {word['word']}: (ì˜ˆì‹œ) This example shows how to use {word['word']} correctly.\n"
        
        content += f"""
---

## Section 4: ë¬¸ë§¥ ë²ˆì—­ ì˜ˆì‹œ ë‹µì•ˆ

"""
        
        contexts = [
            "ë¹„ì¦ˆë‹ˆìŠ¤ íšŒì˜ì—ì„œ",
            "ì¹œêµ¬ì™€ì˜ ì¼ìƒ ëŒ€í™”ì—ì„œ", 
            "ê³µì‹ì ì¸ ì´ë©”ì¼ì—ì„œ",
            "ì¹´í˜ì—ì„œ ì£¼ë¬¸í•  ë•Œ",
            "ì—¬í–‰ ì¤‘ í˜¸í…”ì—ì„œ"
        ]
        
        for i, (word, context) in enumerate(zip(context_words[:5], contexts), 1):
            content += f"{i}. {context} - {word['word']}: (ìƒí™©ì— ë§ëŠ” í‘œí˜„ ì‚¬ìš©)\n"
        
        if synonym_words:
            content += f"""
---

## Section 5: ë™ì˜ì–´ ì •ë‹µ

"""
            
            for i, word in enumerate(synonym_words, 1):
                synonyms_text = ', '.join(word['synonyms'])
                content += f"{i}. {word['word']}: **{synonyms_text}**\n"
        
        return content
    
    def upload_exam_files(self, exam_result: Dict, channel_id: str):
        """ì‹œí—˜ì§€ íŒŒì¼ì„ Slackì— ì—…ë¡œë“œ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ì‹œí—˜ì§€ íŒŒì¼
            exam_filename = f"ì˜ì–´ì‹œí—˜ì§€_{timestamp}.txt"
            with open(exam_filename, 'w', encoding='utf-8') as f:
                f.write(exam_result['exam_content'])
            
            # ë‹µì§€ íŒŒì¼
            answer_filename = f"ì˜ì–´ì‹œí—˜ì§€_ë‹µì§€_{timestamp}.txt"
            with open(answer_filename, 'w', encoding='utf-8') as f:
                f.write(exam_result['answer_content'])
            
            # Slackì— íŒŒì¼ ì—…ë¡œë“œ
            self.client.files_upload_v2(
                channel=channel_id,
                file=exam_filename,
                title="ì˜ì–´ ì‹œí—˜ì§€",
                initial_comment="ğŸ“ ìƒì„±ëœ ì˜ì–´ ì‹œí—˜ì§€ì…ë‹ˆë‹¤."
            )
            
            self.client.files_upload_v2(
                channel=channel_id,
                file=answer_filename,
                title="ì˜ì–´ ì‹œí—˜ì§€ ë‹µì§€", 
                initial_comment="âœ… ì‹œí—˜ì§€ ì •ë‹µì…ë‹ˆë‹¤."
            )
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            import os
            os.remove(exam_filename)
            os.remove(answer_filename)
            
            logger.info("ì‹œí—˜ì§€ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def check_duplicate(self, word: str) -> bool:
        """Google Sheetsì—ì„œ ì¤‘ë³µ ê²€ì‚¬"""
        try:
            cell_list = self.worksheet.col_values(1)
            
            word_lower = word.lower()
            for cell_value in cell_list:
                if cell_value.lower() == word_lower:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            return False
    
    def add_to_sheet(self, word_info: Dict) -> bool:
        """Google Sheetsì— ë‹¨ì–´ ì •ë³´ ì¶”ê°€"""
        try:
            word = word_info.get('word', '')
            
            if self.check_duplicate(word):
                logger.info(f"ì¤‘ë³µ ë‹¨ì–´ ê°ì§€: {word}")
                return False
            
            next_row = len(self.worksheet.col_values(1)) + 1
            
            synonyms = word_info.get('synonyms', [])
            synonyms_text = ', '.join(synonyms) if synonyms else ''
            
            row_data = [
                word_info.get('word', ''),
                word_info.get('korean_meaning', ''),
                synonyms_text
            ]
            
            self.worksheet.insert_row(row_data, next_row)
            logger.info(f"ë‹¨ì–´ ì¶”ê°€ ì„±ê³µ: {word} (í–‰ {next_row})")
            
            return True
            
        except Exception as e:
            logger.error(f"ì‹œíŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def format_vocab_response(self, word_info: Dict, save_success: bool) -> str:
        """ë‹¨ì–´ ì‘ë‹µ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        word = word_info.get('word', '')
        meaning = word_info.get('korean_meaning', '')
        synonyms = word_info.get('synonyms', [])
        example = word_info.get('example', '')
        korean_example = word_info.get('korean_example', '')
        
        response = f"ğŸ“š *{word}*\n"
        response += f"ğŸ‡°ğŸ‡· *ëœ»:* {meaning}\n"
        
        if synonyms:
            response += f"ğŸ”„ *ë™ì˜ì–´:* {', '.join(synonyms)}\n"
        
        if example:
            response += f"\nğŸ’¬ *ì˜ˆë¬¸:*\n> {example}\n"
            if korean_example:
                response += f"> _{korean_example}_\n"
        
        response += "\n" + "="*30 + "\n"
        
        if save_success:
            response += "âœ… *Google Sheetsì— ì €ì¥ ì™„ë£Œ!*"
        else:
            response += "â„¹ï¸ *ì´ë¯¸ ì €ì¥ëœ ë‹¨ì–´ì…ë‹ˆë‹¤.*"
        
        return response
    
    def start(self):
        """ë´‡ ì‹œì‘"""
        logger.info("ì˜ì–´ ë‹¨ì–´ ë´‡ ì‹œì‘...")
        self.socket_handler.start()

class IntegratedEnglishSystem:
    """í†µí•© ì˜ì–´ í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.article_extractor = NYTimesArticleExtractor(
            api_key=config.get('nyt_api_key')
        )
        
        self.ai_processor = AIArticleProcessor(
            anthropic_api_key=config['anthropic_api_key']
        )
        
        self.slack_notifier = SlackNotifier(
            bot_token=config['slack_bot_token'],
            channel=config['slack_channel']
        )
        
        # Slack ë´‡ (ì„ íƒì )
        if config.get('enable_vocab_bot', True):
            if config.get('slack_app_token'):
                self.vocab_bot = VocabBot(
                    slack_bot_token=config['slack_bot_token'],
                    slack_app_token=config['slack_app_token'],
                    anthropic_api_key=config['anthropic_api_key'],
                    google_service_account_file=config['google_service_account_file'],
                    spreadsheet_id=config['google_spreadsheet_id']
                )
            else:
                logger.warning("SLACK_APP_TOKEN not provided. VocabBot will be disabled.")
                self.vocab_bot = None
        else:
            self.vocab_bot = None
    
    def run_daily_pipeline(self) -> Dict:
        """ì¼ì¼ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            logger.info("=== ì˜ì–´ í•™ìŠµ ìë™í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
            
            # Phase 2: ê¸°ì‚¬ ì¶”ì¶œ
            logger.info("Phase 2: ë‰´ìš•íƒ€ì„ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ")
            topics = ['medical', 'politics', 'technology']
            daily_topic = self.article_extractor.get_daily_topic()
            topics.remove(daily_topic)
            topics.insert(0, daily_topic)

            articles = []
            for topic in topics:
                logger.info(f"ì˜¤ëŠ˜ì˜ ì£¼ì œ: {topic}")
                articles = self.article_extractor.extract_articles(topic)
                if articles:
                    break

            if not articles:
                return {'success': False, 'error': 'Phase 2: ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨'}
            
            # Phase 3: AI ì²˜ë¦¬
            logger.info("Phase 3: AI ë²ˆì—­ ë° í•™ìŠµ ìë£Œ ìƒì„±")
            ai_result = self.ai_processor.process_article(articles[0])
            
            if not ai_result.get('success'):
                return {'success': False, 'error': f"Phase 3: {ai_result.get('error')}"}
            
            # Phase 4: Slack ì „ì†¡
            logger.info("Phase 4: Slack í•™ìŠµ ìë£Œ ì „ì†¡")
            slack_result = self.slack_notifier.send_daily_materials(
                ai_result['mixed_content_data'],
                ai_result['commentary_data']
            )
            
            logger.info("=== ì¼ì¼ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            
            return {
                'success': True,
                'topic': daily_topic,
                'article_title': ai_result['article_title'],
                'slack_sent': slack_result['mixed_content_sent'] and slack_result['commentary_sent'],
                'files': {
                    'mixed_content': ai_result['mixed_content_file'],
                    'commentary': ai_result['commentary_file']
                }
            }
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def start_vocab_bot(self):
        """Slack ë‹¨ì–´ ë´‡ ì‹œì‘"""
        if self.vocab_bot:
            logger.info("Slack ë‹¨ì–´ ë´‡ ì‹œì‘")
            self.vocab_bot.start()
        else:
            logger.warning("ë‹¨ì–´ ë´‡ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")

import argparse

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Integrated English Learning System')
    parser.add_argument('--mode', type=str, default='pipeline', choices=['pipeline', 'bot', 'both'], help='Execution mode')
    parser.add_argument('--anthropic-api-key', type=str, help='Anthropic API Key')
    parser.add_argument('--slack-bot-token', type=str, help='Slack Bot Token')
    parser.add_argument('--slack-app-token', type=str, help='Slack App Token')
    parser.add_argument('--slack-channel', type=str, default='#english-learning', help='Slack Channel')
    parser.add_argument('--google-service-account-file', type=str, default='service_account.json', help='Google Service Account File')
    parser.add_argument('--google-spreadsheet-id', type=str, help='Google Spreadsheet ID')
    parser.add_argument('--nyt-api-key', type=str, help='New York Times API Key')
    parser.add_argument('--enable-vocab-bot', type=str, default='true', help='Enable Vocab Bot')

    args = parser.parse_args()

    # UTF-8 ì¸ì½”ë”© ì„¤ì •
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    config = {
        'anthropic_api_key': args.anthropic_api_key or os.getenv('ANTHROPIC_API_KEY'),
        'slack_bot_token': args.slack_bot_token or os.getenv('SLACK_BOT_TOKEN'),
        'slack_app_token': args.slack_app_token or os.getenv('SLACK_APP_TOKEN'),
        'slack_channel': args.slack_channel or os.getenv('SLACK_CHANNEL', '#english-learning'),
        'google_service_account_file': args.google_service_account_file or os.getenv('GOOGLE_SERVICE_ACCOUNT_FILE', 'service_account.json'),
        'google_spreadsheet_id': args.google_spreadsheet_id or os.getenv('GOOGLE_SPREADSHEET_ID'),
        'nyt_api_key': args.nyt_api_key or os.getenv('NYT_API_KEY'),
        'enable_vocab_bot': (args.enable_vocab_bot or os.getenv('ENABLE_VOCAB_BOT', 'true')).lower() == 'true'
    }

    # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì‚¬
    required_vars = ['anthropic_api_key', 'slack_bot_token', 'google_spreadsheet_id']
    missing_vars = [var for var in required_vars if not config[var]]

    if missing_vars:
        logger.error(f"Missing required environment variables: {missing_vars}")
        print("\nRequired environment variables:")
        print("- ANTHROPIC_API_KEY: Claude API Key")
        print("- SLACK_BOT_TOKEN: Slack Bot Token")
        print("- SLACK_APP_TOKEN: Slack App Token (for bot mode)")
        print("- GOOGLE_SPREADSHEET_ID: Google Sheets ID")
        print("- SLACK_CHANNEL: Slack Channel (optional)")
        print("- NYT_API_KEY: New York Times API Key (optional)")
        return

    try:
        system = IntegratedEnglishSystem(config)

        # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
        mode = args.mode

        if mode == 'bot':
            # ë‹¨ì–´ ë´‡ë§Œ ì‹¤í–‰
            print("ğŸ¤– Slack ë‹¨ì–´ ë´‡ ëª¨ë“œë¡œ ì‹¤í–‰")
            system.start_vocab_bot()

        elif mode == 'pipeline':
            # ì¼ì¼ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            print("ğŸ“° ì¼ì¼ í•™ìŠµ ìë£Œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
            result = system.run_daily_pipeline()

            if result['success']:
                print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ!")
                print(f"ì£¼ì œ: {result['topic']}")
                print(f"ê¸°ì‚¬: {result['article_title']}")
                print(f"Slack ì „ì†¡: {'ì„±ê³µ' if result['slack_sent'] else 'ì‹¤íŒ¨'}")
            else:
                print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {result['error']}")

        elif mode == 'both':
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í›„ ë´‡ ì‹œì‘
            print("ğŸ”„ íŒŒì´í”„ë¼ì¸ + ë´‡ í†µí•© ëª¨ë“œ")

            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = system.run_daily_pipeline()
            print(f"íŒŒì´í”„ë¼ì¸ ê²°ê³¼: {'ì„±ê³µ' if result['success'] else 'ì‹¤íŒ¨'}")

            # ë´‡ ì‹œì‘
            system.start_vocab_bot()

        else:
            print("âŒ ì˜ëª»ëœ ì‹¤í–‰ ëª¨ë“œ. RUN_MODEë¥¼ 'pipeline', 'bot', 'both' ì¤‘ í•˜ë‚˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()